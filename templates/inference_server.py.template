#!/usr/bin/env python3
from flask import Flask, request, jsonify
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import traceback

app = Flask(__name__)
MODEL_NAME = "Qwen/Qwen2.5-0.5B-Instruct"
PORT = 8080

# Rocket's identity context
SYSTEM_PROMPT = """You are Rocket, an AI assistant running in a Docker container. You are part of a Matrix chat room with:
- Thomas (human, your creator)
- Aria Prime (another AI assistant)
- Nova (another AI assistant conducting research)

Your purpose is to assist in conversations, answer questions helpfully, and collaborate with the team. You are friendly, concise, and clear in your responses.

Important: You are ROCKET. When someone introduces themselves, acknowledge them but maintain your own identity as Rocket."""

def load_model():
    global model, tokenizer
    print(f"Loading model: {MODEL_NAME}")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        dtype=torch.float32,  # Use dtype instead of torch_dtype
        device_map="cpu"
    )
    model.eval()
    print("Model loaded successfully!")

@app.route("/health", methods=["GET"])
def health():
    return jsonify({"status": "ok", "model": MODEL_NAME})

@app.route("/generate", methods=["POST"])
def generate():
    try:
        data = request.get_json(force=True)
        if data is None:
            return jsonify({"error": "No JSON data received"}), 400
            
        user_prompt = data.get("prompt", "")
        if not user_prompt:
            return jsonify({"error": "No prompt provided"}), 400
            
        max_length = data.get("max_length", 150)
        
        # Build messages with system prompt and user message
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt}
        ]
        
        # Apply chat template
        text = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        inputs = tokenizer([text], return_tensors="pt")
        
        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_length,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Decode only the new tokens (exclude input)
        response = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        
        return jsonify({
            "prompt": user_prompt,
            "response": response.strip()
        })
    except Exception as e:
        print(f"Error in generate: {e}")
        traceback.print_exc()
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    load_model()
    print(f"Starting inference server on port {PORT}...")
    app.run(host="0.0.0.0", port=PORT, debug=False)
