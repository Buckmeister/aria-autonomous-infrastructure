# Rocket Ollama - Deployment
# Runs Ollama inference server + Matrix listener in a single pod

apiVersion: apps/v1
kind: Deployment
metadata:
  name: rocket-ollama
  namespace: rocket
  labels:
    app: rocket-ollama
    component: inference
spec:
  replicas: 1  # Single instance for now
  selector:
    matchLabels:
      app: rocket-ollama
  template:
    metadata:
      labels:
        app: rocket-ollama
        component: inference
    spec:
      # Container definitions
      containers:
        # Ollama Server - Handles inference
        - name: ollama-server
          image: ollama/ollama:latest
          imagePullPolicy: Always
          ports:
            - name: ollama-api
              containerPort: 11434
              protocol: TCP
          volumeMounts:
            - name: ollama-models
              mountPath: /root/.ollama
          envFrom:
            - configMapRef:
                name: rocket-ollama-config
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -e
              echo "Starting Ollama server..."
              /bin/ollama serve &
              OLLAMA_PID=$!

              # Wait for server to start
              echo "Waiting for Ollama server to start..."
              sleep 10

              # Pull the model if MODEL_NAME is set
              if [ -n "${MODEL_NAME}" ]; then
                echo "Pulling model: ${MODEL_NAME}..."
                ollama pull "${MODEL_NAME}"
              else
                echo "No MODEL_NAME specified, using default (qwen2.5:0.5b)"
                ollama pull qwen2.5:0.5b
              fi

              # Keep server running
              echo "Ollama server ready!"
              wait $OLLAMA_PID
          # Readiness probe - check if Ollama API is responding
          readinessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          # Liveness probe - ensure Ollama is still running
          livenessProbe:
            httpGet:
              path: /api/tags
              port: 11434
            initialDelaySeconds: 120  # Allow time for model download
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "2Gi"
              cpu: "1000m"

        # Matrix Listener - Connects Matrix to Ollama
        - name: matrix-listener
          image: ubuntu:22.04  # Base image for listener
          imagePullPolicy: Always
          envFrom:
            - configMapRef:
                name: rocket-ollama-config
          env:
            # Matrix credentials from secret
            - name: MATRIX_TOKEN
              valueFrom:
                secretKeyRef:
                  name: matrix-credentials
                  key: token
            - name: MATRIX_SERVER
              valueFrom:
                secretKeyRef:
                  name: matrix-credentials
                  key: server
            - name: MATRIX_USER
              valueFrom:
                secretKeyRef:
                  name: matrix-credentials
                  key: user
            - name: MATRIX_ROOM
              valueFrom:
                secretKeyRef:
                  name: matrix-credentials
                  key: room
            # Point to local Ollama server
            - name: INFERENCE_URL
              value: "http://localhost:11434/v1/chat/completions"
          command: ["/bin/bash", "-c"]
          args:
            - |
              set -e
              echo "Installing dependencies for Matrix listener..."
              apt-get update && apt-get install -y curl jq netcat-openbsd

              # Wait for Ollama to be ready
              echo "Waiting for Ollama server to be ready..."
              while ! nc -z localhost 11434; do
                echo "Waiting for Ollama..."
                sleep 5
              done
              echo "Ollama is ready!"

              # Run the listener script
              echo "Starting Matrix conversational listener..."
              # TODO: Mount and execute actual listener script
              # For now, keep container alive for debugging
              echo "Listener script needs to be mounted from repo"
              tail -f /dev/null
          resources:
            requests:
              memory: "256Mi"
              cpu: "250m"
            limits:
              memory: "512Mi"
              cpu: "500m"

      # Volumes
      volumes:
        - name: ollama-models
          persistentVolumeClaim:
            claimName: ollama-models

      # Restart policy
      restartPolicy: Always

---
# Note: Matrix listener script mounting
#
# To properly mount the listener script, you'll need to either:
#
# 1. Build a custom Docker image with the script:
#    - Create a Dockerfile based on ubuntu:22.04
#    - COPY the listener script into the image
#    - Update the matrix-listener container to use this image
#
# 2. Use a ConfigMap with the full script:
#    - Create a ConfigMap with the listener script content
#    - Mount it as a volume in the matrix-listener container
#
# 3. Use hostPath to mount from the node (dev only):
#    - Add a volume with hostPath pointing to your repo
#    - Mount it in the matrix-listener container
#
# For production, option 1 (custom image) is recommended.
